Disk I/O can be much slower than other aspects of the system. Because I/O scheduling allows you to store events and possibly reorder them, it’s possible to produce contiguous I/O requests to improve performance. Newer filesystems are incorporating some of these concepts, and you can even extend these concepts to make the system better adapt to the properties of SSDs.

I/O schedulers typically use the following techniques:
1. Request Merging: Adjacent requests are merged to reduce disk seeking and to increase the size of the I/O syscalls (usually resulting in higher performance).
2. Elevator: Requests are ordered on the basis of physical location on the disk so that seeks are in one direction as much as possible. This technique is sometimes
   referred to as sorting.
3. Prioritization: Requests are prioritized in some way. The details of the ordering are up to the I/O scheduler.

Linux I/O Schedulers
1. Completely Fair Queuing (CFQ)
2. Deadline
3. NOOP
4. Anticipatory

1. Deadline Scheduler
The deadline elevator is a cyclic elevator (round robin) with a deadline algorithm that provides a near real-time behavior of the I/Osubsystem.
The implementation of the deadline algorithm ensures that starvation of a process cannot occur.
There are 3 Queues
1.Linux Elevator Queue -> Linus Elevator maintains the sorted list of pending I/O requests in a single queue. The I/O request at the head of the queue is the next one
to be serviced.
2.Read FIFO queue
3.Write FIFO queue
The Deadline I/O Scheduler keeps the items in each of these queues sorted by submission time (effectively, first in is first out).
The read FIFO queue, as its name suggests, contains only read requests.
The write FIFO queue, likewise, contains only write requests.
Each FIFO queue is assigned an expiration value. The read FIFO queue has an expiration time of 500 milliseconds.
The write FIFO queue has an expiration time of 5seconds.
When a new I/O request is submitted, it is insertion-sorted into the standard Elevator queue and placed at the tail of its respective (either read or write)
FIFO queue.Normally, the hard drive is sent I/O requests from the head of the standard sorted queue. This maximizes global throughput by minimizing seeks, as the normal queue is sorted by block number, as with the Linus Elevator.
When the item at the head of one of the FIFO queues, however, grows older than the expiration value associated with its queue, the I/O scheduler stops dispatching I/O requests from the standard queue. Instead, it services the I/O request at the head of the FIFO queue, plus a couple extra for good measure. The I/O scheduler needs to check and handle only the requests at the head of the FIFO queues, as those are the oldest requests in the queue.
Deadline dispatches I/Os in batches. A batch is a sequence of either read or write I/Os which are in increasing LBA order (the one-way elevator).
After processing each batch, the I/O scheduler checks to see whether write requests have been starved for too long, and then decides whether to start a new 
batch of reads or writes. The FIFO list of requests is only checked for expired requests at the start of each batch, and then only for the data direction of thebatch. So, if a write batch is selected, and there is an expired read request, that read request will not get serviced until the write batch completes.
In this manner, the Deadline I/O Scheduler can enforce a soft deadline on I/O requests.
Although it makes no promise that an I/O request is serviced before the expiration time, the I/O scheduler generally services requests near their expiration 
times. 
Thus, the Deadline I/O Scheduler continues to provide good global throughput without starving any one request for an unacceptably long time. Because read
requests are given short expiration times, the writes-starving-reads problem is minimized.
A deadline scheduler really helps with distant reads (i.e., fairly far out on the disk or with a large sector number).
Read I/O requests sometimes block applications because they have to be executed while the application waits.
On the other hand, because writes are cached, execution can quickly return to the application – unless you have turned off the cache in the interest of making sure the data reaches the disk in the event of a power loss, in which case, writes would behave like read requests.
Even worse, distant reads would be serviced very slowly because they are constantly moved to the back of the queue as requests for closer parts of the disk are servicedfirst. However, a deadline I/O scheduler makes sure that all I/O requests are serviced, even the distant read requests.
The deadline scheduler is very useful for some applications. In particular, real-time systems use the deadline scheduler because, in most cases, it keeps latency low (all requests are serviced within a short time frame). It has also been suggested that it works well for database systems that have TCQ-aware disks.
The deadline I/O scheduler attempts to provide a guaranteed latency for requests.


Anticipatory I/O Scheduler 
The anticipatory I/O scheduler was the default scheduler a long time ago (in kernel years). 
As the name implies, it anticipates subsequent block requests and implements request merging, a one-way elevator (a simple elevator), and read and write request
batching. 
After the scheduler services an I/O request, it anticipates that the next request will be for the subsequent block. If the request comes, the disk head is in the
correct location, and the request is serviced very quickly. 
This approach does add a little latency to the system because it pauses slightly to see if the next request is for the subsequent block. However, this latency is possibly outweighed by the increased performance for neighboring requests.
Putting on your storage expert hat, you can see that the Anticipatory scheduler works really well for certain workloads. For example, one study observed that the 
Apache web server could achieve up to 71% more throughput using the anticipatory I/O scheduler. On the other hand, the anticipatory scheduler has been observed to 
result in a slowdown on a database run.

CFQ I/O Scheduler (Completely Fair Scheduler)
The completely fair queue (CFQ) I/O scheduler, is the current default scheduler in the Linux kernel. It uses both request merging and elevators and is a bit more complex that the NOOP or deadline schedulers.
CFQ synchronously puts requests from processes into a number of per-process queues then allocates time slices for each of the queues to access the disk.
The details of the length of the time slice and the number of requests a queue is allowed to submit are all dependent on the I/O priority of the given process. 
Asynchronous requests for all processes are batched together into fewer queues with one per priority.
CFQ gives all users (processes) of a particular device (storage) about the same number of I/O requests over a particular time interval, which can help multiuser 
systems see that all users get about the same level of responsiveness.
Moreover, CFQ achieves some of the good throughput characteristics of the anticipatory scheduler because it allows a process queue to have some idle time at the end of a synchronous I/O request, creating some anticipatory time for I/O that might be close to the request just serviced.
The CFQ elevator implements a QoS (Quality of Service) policy for processes by maintaining per-process I/O queues. The CFQ elevator is well suited for large multiuser systems with a lot of competing processes
Starting with kernel release 2.6.18 the improved CFQ elevator is the default I/O scheduler. Depending on the system setup and the workload characteristics, the CFQ scheduler can slowdown a single main application, for example a massive database with its fairness oriented algorithms.

NOOP
The NOOP I/O scheduler is fairly simple.
All incoming I/O requests for all processes running on the system, regardless of the I/O request (e.g., read, write, lseek, etc.), go into a simple first in, first out (FIFO) queue. The scheduler also does request merging by taking adjacent requests and merging them into a single request to reduce seek time and improve throughput.
NOOP assumes that some other device will optimize I/O performance, such as an external RAID controller or a SAN controller.

Potentially, the NOOP scheduler could work well with storage devices that don’t have a mechanical component (i.e., a drive head) to read data, because it does not make any attempts to reduce seek time beyond simple request merging (which helps throughput). Therefore, storage devices such as flash drives, SSD drives, USB sticks, and the like that have very little seek time could benefit from a NOOP I/O scheduler.
